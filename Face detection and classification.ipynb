{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9942f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3e580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, BatchNormalization, Activation\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.models import Sequential, Model\n",
    "from  matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9077e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28de147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder=\"C:/Users/Amarir/Downloads/for research 1/Practical part/READY\"\n",
    "face_cascade=cv2.CascadeClassifier(\"C:/Users/Amarir/Downloads/haarcascade_frontalface_default.xml\")\n",
    "height=200\n",
    "width=200,\n",
    "image_size=(height,width,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330793e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Amarir/Downloads/for research 1/Practical part/READY')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "img_folder = pathlib.Path(img_folder)\n",
    "img_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deeb3b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/Amarir/Downloads/for research 1/Practical part/READY/S01/s01_01.jpg'),\n",
       " WindowsPath('C:/Users/Amarir/Downloads/for research 1/Practical part/READY/S01/s01_02.jpg'),\n",
       " WindowsPath('C:/Users/Amarir/Downloads/for research 1/Practical part/READY/S01/s01_03.jpg'),\n",
       " WindowsPath('C:/Users/Amarir/Downloads/for research 1/Practical part/READY/S01/s01_04.jpg'),\n",
       " WindowsPath('C:/Users/Amarir/Downloads/for research 1/Practical part/READY/S01/s01_05.jpg')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(img_folder.glob('*/*.jpg'))[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe048b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(img_folder):\n",
    "   \n",
    "    img_data_array=[]\n",
    "    class_name=[]\n",
    "    classes=[]\n",
    "   \n",
    "    for dir1 in os.listdir(img_folder):\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\n",
    "       \n",
    "            image_path= os.path.join(img_folder, dir1,  file)\n",
    "            image= cv2.imread( image_path, cv2.COLOR_BGR2RGB)\n",
    "            image=cv2.resize(image, (200, 200),interpolation = cv2.INTER_AREA)\n",
    "            image=np.array(image)\n",
    "            image = image.astype('float32')\n",
    "            image /= 255 \n",
    "            img_data_array.append(image)\n",
    "            class_name.append(dir1)\n",
    "            if dir1 not in classes:\n",
    "                classes.append(dir1)\n",
    "\n",
    "    return img_data_array, class_name,classes\n",
    "# extract the image array and class name\n",
    "img_data, class_name,classes =create_dataset(\"C:/Users/Amarir/Downloads/for research 1/Practical part/READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09ad1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict={k: v for v, k in enumerate(np.unique(class_name))}\n",
    "target_val=  [target_dict[class_name[i]] for i in range(len(class_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb8dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dict = {\n",
    "    'S01': list(img_folder.glob('S01/*')),\n",
    "    'S02': list(img_folder.glob('S02/*')),\n",
    "    'S03': list(img_folder.glob('S03/*')),\n",
    "    'S04': list(img_folder.glob('S04/*')),\n",
    "    'S05': list(img_folder.glob('S05/*')),\n",
    "    'S06': list(img_folder.glob('S06/*')),\n",
    "    'S07': list(img_folder.glob('S07/*')),\n",
    "    'S08': list(img_folder.glob('S08/*')),\n",
    "    'S09': list(img_folder.glob('S09/*')),\n",
    "    'S10': list(img_folder.glob('S10/*')),\n",
    "\n",
    "    'S11': list(img_folder.glob('S11/*')),\n",
    "    'S12': list(img_folder.glob('S12/*')),\n",
    "    'S13': list(img_folder.glob('S13/*')),\n",
    "    'S14': list(img_folder.glob('S14/*')),\n",
    "    'S15': list(img_folder.glob('S15/*')),\n",
    "    'S16': list(img_folder.glob('S16/*')),\n",
    "    'S17': list(img_folder.glob('S17/*')),\n",
    "    'S18': list(img_folder.glob('S18/*')),\n",
    "    'S19': list(img_folder.glob('S19/*')),\n",
    "    'S20': list(img_folder.glob('S20/*')),\n",
    "\n",
    "    'S21': list(img_folder.glob('S21/*')),\n",
    "    'S22': list(img_folder.glob('S22/*')),\n",
    "    'S23': list(img_folder.glob('S23/*')),\n",
    "    'S24': list(img_folder.glob('S24/*')),\n",
    "    'S25': list(img_folder.glob('S25/*')),\n",
    "    'S26': list(img_folder.glob('S26/*')),\n",
    "    'S27': list(img_folder.glob('S27/*')),\n",
    "    'S28': list(img_folder.glob('S28/*')),\n",
    "    'S29': list(img_folder.glob('S29/*')),\n",
    "    'S30': list(img_folder.glob('S30/*')),\n",
    "\n",
    "    'S31': list(img_folder.glob('S31/*')),\n",
    "    'S32': list(img_folder.glob('S32/*')),\n",
    "    'S33': list(img_folder.glob('S33/*')),\n",
    "    'S34': list(img_folder.glob('S34/*')),\n",
    "    'S35': list(img_folder.glob('S35/*')),\n",
    "    'S36': list(img_folder.glob('S36/*')),\n",
    "    'S37': list(img_folder.glob('S37/*')),\n",
    "    'S38': list(img_folder.glob('S38/*')),\n",
    "    'S39': list(img_folder.glob('S39/*')),\n",
    "    'S40': list(img_folder.glob('S40/*')),\n",
    "\n",
    "    'S41': list(img_folder.glob('S41/*')),\n",
    "    'S42': list(img_folder.glob('S42/*')),\n",
    "    'S43': list(img_folder.glob('S43/*')),\n",
    "    'S44': list(img_folder.glob('S44/*')),\n",
    "    'S45': list(img_folder.glob('S45/*')),\n",
    "    'S46': list(img_folder.glob('S46/*')),\n",
    "    'S47': list(img_folder.glob('S47/*')),\n",
    "    'S48': list(img_folder.glob('S48/*')),\n",
    "    'S49': list(img_folder.glob('S49/*')),\n",
    "    'S50': list(img_folder.glob('S50/*')),\n",
    "    \n",
    "    'safa': list(img_folder.glob('safa/*')),\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7d2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for class_name, images in images_dict.items():\n",
    "    for image in images:\n",
    "        img = cv2.imread(str(image))\n",
    "        resized_img = cv2.resize(img,(200,200))\n",
    "        X.append(resized_img)\n",
    "        y.append(target_dict[class_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c29e11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d3e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41ad72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.preprocessing.image_preprocessing import RandomContrast\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\",input_shape=(200,200,3)),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomContrast(0.6),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a76bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    data_augmentation,\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(200,200,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(51, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e0e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cfb1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'C:/Users/Amarir/Downloads/for research 1/Practical part/new weights/Alexnet_GT20_80my_face.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "007cfe0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x247ffa11dc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20b2aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ped(img,label, x0, y0, xt, yt, color=(255,127,0), text_color=(255,255,255)):\n",
    "\n",
    "    (w, h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "    cv2.rectangle(img,(x0, y0 + baseline),  (max(xt, x0 + w), yt),  color, 2)\n",
    "    cv2.rectangle(img,(x0, y0 - h),  (x0 + w, y0 + baseline), color, -1)  \n",
    "    cv2.putText(img, label, (x0, y0),cv2.FONT_HERSHEY_SIMPLEX,0.5,text_color,1,cv2.LINE_AA) \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60853211",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=np.flip(classes,0)\n",
    "classes=np.sort(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d86c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text=\"\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened() :\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        color = cv2.cvtColor(frame, cv2.COLOR_RGBA2RGB)\n",
    "        faces = face_cascade.detectMultiScale(color, 1.1, 5)\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = gray[y:y+h, x:x+w]\n",
    "            face_img = cv2.resize(face_img,(200,200))\n",
    "            face_img = img_to_array(face_img)\n",
    "            face_img = np.expand_dims(face_img, axis=0)\n",
    "            \n",
    "            confidence = model.predict(face_img)[0]\n",
    "            idx = np.argmax(confidence)\n",
    "            label = classes[idx]\n",
    "            label_text = \"%s (%.2f %%)\" % (label, confidence[idx]*100)\n",
    "            \n",
    "            frame = draw_ped(frame,label_text, x, y, x + w, y + h, color=(0,255,255), text_color=(50,50,50))\n",
    "       \n",
    "        cv2.imshow('Detect Face', frame)\n",
    "    else :\n",
    "        break\n",
    "    if cv2.waitKey(10) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c22e256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c53341d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
